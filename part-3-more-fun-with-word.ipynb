{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"},{"sourceId":9665276,"sourceType":"datasetVersion","datasetId":5905595}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! unzip /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n! unzip /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n! unzip /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:54:38.843874Z","iopub.execute_input":"2024-10-24T18:54:38.844967Z","iopub.status.idle":"2024-10-24T18:54:43.802477Z","shell.execute_reply.started":"2024-10-24T18:54:38.844917Z","shell.execute_reply":"2024-10-24T18:54:43.800938Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Archive:  /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n  inflating: labeledTrainData.tsv    \nArchive:  /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n  inflating: testData.tsv            \nArchive:  /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n  inflating: unlabeledTrainData.tsv  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"Part 2中训练好的word2vec模型，对于每一个词都有一个特征向量，用numpy存储，可以通过syn0调用","metadata":{}},{"cell_type":"code","source":"# Load the model that we created in Part 2\nfrom gensim.models import Word2Vec\nmodel = Word2Vec.load(\"/kaggle/input/inputdata/300features_40minwords_10context\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:55:06.490224Z","iopub.execute_input":"2024-10-24T18:55:06.490766Z","iopub.status.idle":"2024-10-24T18:55:17.353201Z","shell.execute_reply.started":"2024-10-24T18:55:06.490709Z","shell.execute_reply":"2024-10-24T18:55:17.351860Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model.wv.vectors.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:55:24.970106Z","iopub.execute_input":"2024-10-24T18:55:24.970671Z","iopub.status.idle":"2024-10-24T18:55:24.980579Z","shell.execute_reply.started":"2024-10-24T18:55:24.970552Z","shell.execute_reply":"2024-10-24T18:55:24.979311Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(16490, 300)"},"metadata":{}}]},{"cell_type":"markdown","source":"syn0的每一行，即代表词汇表中的一个单词，即有16490个单词。列代表特征向量的大小，即300，这个是我们在part 2训练时设定的数字。我们设置的最小单词频度是40（即出现40次以下的单词会被忽略），最后得到一个有16492个单词的词汇表，每个词有300个特征。\n\n单个词向量可以通过下面的方法查看","metadata":{}},{"cell_type":"code","source":"model.wv['flower'][:20]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:56:26.715740Z","iopub.execute_input":"2024-10-24T18:56:26.716229Z","iopub.status.idle":"2024-10-24T18:56:26.726561Z","shell.execute_reply.started":"2024-10-24T18:56:26.716181Z","shell.execute_reply":"2024-10-24T18:56:26.725187Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([-0.23710842,  0.08830173, -0.03082926,  0.1317306 ,  0.01820597,\n        0.25649256,  0.12806156,  0.20457655,  0.28028506, -0.51132834,\n       -0.09054958,  0.11458612,  0.07447369,  0.33142266, -0.30471832,\n       -0.35287043, -0.18231627, -0.4457859 , -0.02176598,  0.31776565],\n      dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"这里导入之前的数据集：","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(\"/kaggle/working/labeledTrainData.tsv\", header=0, \n                     delimiter=\"\\t\", quoting=3)\n\ntest = pd.read_csv( \"/kaggle/working/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv(\"/kaggle/working/unlabeledTrainData.tsv\", header=0, \n                              delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" \n      % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size ))","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:56:44.555141Z","iopub.execute_input":"2024-10-24T18:56:44.555619Z","iopub.status.idle":"2024-10-24T18:56:46.414373Z","shell.execute_reply.started":"2024-10-24T18:56:44.555561Z","shell.execute_reply":"2024-10-24T18:56:46.412969Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"导入之前的文本预处理函数：","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist(review, remove_stopwords=False):\n    # 函数用于将文档转换成单词序列，可选择是否移除停用词。返回一个单词列表。\n    \n    # 1. 移除HTML标签\n    review_text = BeautifulSoup(review, 'lxml').get_text()\n      \n    # 2. 移除非字母字符\n    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n    \n    # 3. 将所有单词转换为小写并分割它们\n    words = review_text.lower().split()\n\n    # 4. 可选择是否移除停用词（默认为False）\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))  # 从nltk库中加载英文停用词列表，并转换为集合\n        words = [w for w in words if not w in stops]  # 移除停用词\n    \n    # 5. 返回单词列表\n    return words","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:57:49.920718Z","iopub.execute_input":"2024-10-24T18:57:49.921175Z","iopub.status.idle":"2024-10-24T18:57:51.162121Z","shell.execute_reply.started":"2024-10-24T18:57:49.921132Z","shell.execute_reply":"2024-10-24T18:57:51.160693Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"导入之前的函数：使用NLTK库的punkt分词器将一段评论文本分割成句子，并将每个句子进一步处理成单词列表。","metadata":{}},{"cell_type":"code","source":"import nltk.data\n# nltk.download()   \n\n# 加载punkt分词器\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# 定义一个函数，将评论分割成解析后的句子\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # 这个函数将评论分割成解析后的句子。返回一个句子列表，其中每个句子是一个单词列表\n    \n    # 1. 使用NLTK分词器将段落分割成句子\n    raw_sentences = tokenizer.tokenize(review.strip())\n    \n    # 2. 循环遍历每个句子\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # 如果一个句子为空，跳过它\n        if len(raw_sentence) > 0:\n            # 否则，调用review_to_wordlist函数来获取单词列表\n            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n\n    # 返回句子列表（每个句子是一个单词列表，\n    # 因此，这返回一个列表的列表\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:59:50.950776Z","iopub.execute_input":"2024-10-24T18:59:50.951174Z","iopub.status.idle":"2024-10-24T18:59:50.958919Z","shell.execute_reply.started":"2024-10-24T18:59:50.951139Z","shell.execute_reply":"2024-10-24T18:59:50.957477Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**由单词到段落，尝试1：向量平均**\n\n电影评论数据集处理起来一个比较麻烦的地方在于，评论的长度是不一样的。我们需要提取出每一个词的向量，然后把它们转换为一个特征集，而且每个评论的特征长度是一样的。\n\n\n因为每一个单词有一个300维的特征，我们可以用特征操作来把一个评论中的单词合并起来。一个简单的方法就是对所有的词向量取平均。（如果取平均的话，我们需要移除stop words，因为会带来噪音）\n\n\n下面是计算特征向量平均值的代码：","metadata":{}},{"cell_type":"code","source":"import numpy as np \n\ndef makeFeatureVec(words, model, num_features):\n    # 函数用于平均给定段落中的所有词向量\n    # 预初始化一个空的numpy数组（为了速度）\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    \n    nwords = 0\n    \n    # index2word是包含模型词汇表中单词名称的列表。将其转换为集合，以提高速度\n    index2word_set = set(model.wv.index_to_key)\n    \n    # 循环遍历评论中的每个词，如果它在模型的词汇表中，\n    # 将其特征向量加到总数中\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1\n            featureVec = np.add(featureVec, model.wv[word])\n    \n    # 将结果除以单词数量以得到平均值\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # 给定一组评论（每个评论是一个单词列表），计算每个评论的平均特征向量并返回一个2D numpy数组 \n    \n    # 初始化一个计数器\n    counter = 0\n    \n    # 预分配一个2D numpy数组，为了速度\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    \n    # 循环遍历评论\n    for review in reviews:\n       \n        # 每处理1000个评论打印一次状态消息\n        if counter%1000 == 0.:\n            print(\"Review %d of %d\" % (counter, len(reviews)))\n       \n        # 调用上面定义的函数，生成平均特征向量\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n       \n        # 增加计数器\n        counter = counter + 1\n    return reviewFeatureVecs","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:03:13.560651Z","iopub.execute_input":"2024-10-24T19:03:13.561124Z","iopub.status.idle":"2024-10-24T19:03:13.572267Z","shell.execute_reply.started":"2024-10-24T19:03:13.561082Z","shell.execute_reply":"2024-10-24T19:03:13.570671Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"调用上面的函数来给每一个评论创建一个平均向量。","metadata":{}},{"cell_type":"code","source":"# 设置各种参数的值\nnum_features = 300    # 词向量的维度\nmin_word_count = 40   # 最低词频\nnum_workers = 4       # 并行运行的线程数量\ncontext = 10          # 上下文窗口大小\ndownsampling = 1e-3   # 频繁词的降采样设置","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:04:08.210327Z","iopub.execute_input":"2024-10-24T19:04:08.210785Z","iopub.status.idle":"2024-10-24T19:04:08.216601Z","shell.execute_reply.started":"2024-10-24T19:04:08.210744Z","shell.execute_reply":"2024-10-24T19:04:08.215457Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# ****************************************************************\n# 计算训练集的平均特征向量\n\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    # 对每个训练集评论调用review_to_wordlist函数，并移除停用词\n    clean_train_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n\ntrainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n# 输出消息，表示正在为测试集评论创建平均特征向量\nprint(\"Creating average feature vecs for test reviews\")\n\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    # 对每个测试集评论调用review_to_wordlist函数，并移除停用词\n    clean_test_reviews.append( review_to_wordlist( review, remove_stopwords=True ))\n\ntestDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:05:40.006318Z","iopub.execute_input":"2024-10-24T19:05:40.007495Z","iopub.status.idle":"2024-10-24T19:07:13.935206Z","shell.execute_reply.started":"2024-10-24T19:05:40.007447Z","shell.execute_reply":"2024-10-24T19:07:13.934070Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/592310003.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  review_text = BeautifulSoup(review, 'lxml').get_text()\n","output_type":"stream"},{"name":"stdout","text":"Review 0 of 25000\nReview 1000 of 25000\nReview 2000 of 25000\nReview 3000 of 25000\nReview 4000 of 25000\nReview 5000 of 25000\nReview 6000 of 25000\nReview 7000 of 25000\nReview 8000 of 25000\nReview 9000 of 25000\nReview 10000 of 25000\nReview 11000 of 25000\nReview 12000 of 25000\nReview 13000 of 25000\nReview 14000 of 25000\nReview 15000 of 25000\nReview 16000 of 25000\nReview 17000 of 25000\nReview 18000 of 25000\nReview 19000 of 25000\nReview 20000 of 25000\nReview 21000 of 25000\nReview 22000 of 25000\nReview 23000 of 25000\nReview 24000 of 25000\nCreating average feature vecs for test reviews\nReview 0 of 25000\nReview 1000 of 25000\nReview 2000 of 25000\nReview 3000 of 25000\nReview 4000 of 25000\nReview 5000 of 25000\nReview 6000 of 25000\nReview 7000 of 25000\nReview 8000 of 25000\nReview 9000 of 25000\nReview 10000 of 25000\nReview 11000 of 25000\nReview 12000 of 25000\nReview 13000 of 25000\nReview 14000 of 25000\nReview 15000 of 25000\nReview 16000 of 25000\nReview 17000 of 25000\nReview 18000 of 25000\nReview 19000 of 25000\nReview 20000 of 25000\nReview 21000 of 25000\nReview 22000 of 25000\nReview 23000 of 25000\nReview 24000 of 25000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"使用随机森林分类器对训练数据进行拟合，并在测试数据上进行预测，最后将预测结果保存","metadata":{}},{"cell_type":"code","source":"# 初始化一个随机森林分类器，使用100棵树\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier( n_estimators = 100 )\n\nprint(\"Fitting a random forest to labeled training data...\")\n# 使用训练数据的特征向量和情感标签来拟合随机森林模型\nforest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n\n# 在测试数据上进行预测并提取结果\nresult = forest.predict( testDataVecs )\n\n# 将测试数据的id和预测结果组合成一个新的DataFrame\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\noutput.to_csv( \"/kaggle/working/submission.csv\", index=False, quoting=3 )","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:08:23.335486Z","iopub.execute_input":"2024-10-24T19:08:23.335964Z","iopub.status.idle":"2024-10-24T19:09:19.775341Z","shell.execute_reply.started":"2024-10-24T19:08:23.335921Z","shell.execute_reply":"2024-10-24T19:09:19.774192Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Fitting a random forest to labeled training data...\n","output_type":"stream"}]}]}