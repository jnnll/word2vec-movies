{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:54:19.022295Z","iopub.execute_input":"2024-12-04T06:54:19.024758Z","iopub.status.idle":"2024-12-04T06:54:29.059078Z","shell.execute_reply.started":"2024-12-04T06:54:19.024702Z","shell.execute_reply":"2024-12-04T06:54:29.058034Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n\nimport sys\nimport logging\nimport datasets\nimport evaluate\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import BertTokenizerFast, DataCollatorWithPadding\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import BertPreTrainedModel, BertModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:54:36.699322Z","iopub.execute_input":"2024-12-04T06:54:36.699943Z","iopub.status.idle":"2024-12-04T06:54:56.135468Z","shell.execute_reply.started":"2024-12-04T06:54:36.699893Z","shell.execute_reply":"2024-12-04T06:54:56.134729Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"! unzip /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n! unzip /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n! unzip /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:55:24.377423Z","iopub.execute_input":"2024-12-04T06:55:24.378449Z","iopub.status.idle":"2024-12-04T06:55:29.161447Z","shell.execute_reply.started":"2024-12-04T06:55:24.378413Z","shell.execute_reply":"2024-12-04T06:55:29.160585Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Archive:  /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n  inflating: labeledTrainData.tsv    \nArchive:  /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n  inflating: testData.tsv            \nArchive:  /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n  inflating: unlabeledTrainData.tsv  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/working/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ntest = pd.read_csv(\"/kaggle/working/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:56:02.514520Z","iopub.execute_input":"2024-12-04T06:56:02.514951Z","iopub.status.idle":"2024-12-04T06:56:03.268413Z","shell.execute_reply.started":"2024-12-04T06:56:02.514919Z","shell.execute_reply":"2024-12-04T06:56:03.267676Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def KL(input, target, reduction=\"sum\"):\n    input = input.float()\n    target = target.float()\n    loss = F.kl_div(F.log_softmax(input, dim=-1, dtype=torch.float32),\n                    F.softmax(target, dtype=torch.float32), reduction=reduction)\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:56:26.628902Z","iopub.execute_input":"2024-12-04T06:56:26.629234Z","iopub.status.idle":"2024-12-04T06:56:26.634200Z","shell.execute_reply.started":"2024-12-04T06:56:26.629207Z","shell.execute_reply":"2024-12-04T06:56:26.633351Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class BertScratch(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.bert = BertModel(config)\n        classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.post_init()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        kl_outputs = self.bert(input_ids, attention_mask, token_type_ids)\n        kl_output = kl_outputs[1]\n        kl_output = self.dropout(kl_output)\n        kl_logits = self.classifier(kl_output)\n\n        total_loss = None\n        if labels is not None:\n            # 实例化损失函数\n            loss_fct = nn.CrossEntropyLoss()\n            # logits和labels的交叉熵损失\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            # kl_logits和labels的交叉熵损失\n            ce_loss = loss_fct(kl_logits.view(-1, self.num_labels), labels.view(-1))\n            # KL损失\n            kl_loss = (KL(logits, kl_logits, \"sum\") + KL(kl_logits, logits, \"sum\")) / 2.\n            # 损失总和\n            total_loss = loss + ce_loss + kl_loss\n\n        return SequenceClassifierOutput(\n            loss=total_loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:56:43.176732Z","iopub.execute_input":"2024-12-04T06:56:43.177045Z","iopub.status.idle":"2024-12-04T06:56:43.185603Z","shell.execute_reply.started":"2024-12-04T06:56:43.177020Z","shell.execute_reply":"2024-12-04T06:56:43.184665Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"program = os.path.basename(sys.argv[0])\nlogger = logging.getLogger(program)\n\nlogging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\nlogging.root.setLevel(level=logging.INFO)\nlogger.info(r\"running %s\" % ''.join(sys.argv))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:57:30.564800Z","iopub.execute_input":"2024-12-04T06:57:30.565706Z","iopub.status.idle":"2024-12-04T06:57:30.571885Z","shell.execute_reply.started":"2024-12-04T06:57:30.565668Z","shell.execute_reply":"2024-12-04T06:57:30.571026Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train, val = train_test_split(train, test_size=.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:57:55.070365Z","iopub.execute_input":"2024-12-04T06:57:55.070731Z","iopub.status.idle":"2024-12-04T06:57:55.091301Z","shell.execute_reply.started":"2024-12-04T06:57:55.070696Z","shell.execute_reply":"2024-12-04T06:57:55.090680Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dict = {'label': train[\"sentiment\"], 'text': train['review']}\nval_dict = {'label': val[\"sentiment\"], 'text': val['review']}\ntest_dict = {\"text\": test['review']}\n\ntrain_dataset = datasets.Dataset.from_dict(train_dict)\nval_dataset = datasets.Dataset.from_dict(val_dict)\ntest_dataset = datasets.Dataset.from_dict(test_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:58:09.017416Z","iopub.execute_input":"2024-12-04T06:58:09.017994Z","iopub.status.idle":"2024-12-04T06:58:09.707437Z","shell.execute_reply.started":"2024-12-04T06:58:09.017962Z","shell.execute_reply":"2024-12-04T06:58:09.706506Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples['text'], truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:58:24.858316Z","iopub.execute_input":"2024-12-04T06:58:24.858689Z","iopub.status.idle":"2024-12-04T06:58:29.078512Z","shell.execute_reply.started":"2024-12-04T06:58:24.858661Z","shell.execute_reply":"2024-12-04T06:58:29.077793Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"518a9cda96a14ea093bceb93712f7dc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"535dbe5dc5da4d918a49ac88617fb42e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c41da9d2524692b78561b7eb8555e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb2e138354f646eebf1eb70a4602019e"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"tokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_val = val_dataset.map(preprocess_function, batched=True)\ntokenized_test = test_dataset.map(preprocess_function, batched=True)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nmodel = BertScratch.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:58:44.174276Z","iopub.execute_input":"2024-12-04T06:58:44.174645Z","iopub.status.idle":"2024-12-04T06:59:12.058685Z","shell.execute_reply.started":"2024-12-04T06:58:44.174614Z","shell.execute_reply":"2024-12-04T06:59:12.058066Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bd02e4e630946b3b835034546226c02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d24d30cf4664443a171e159437240e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c446a097ed74b95bdd6a31c92f37d87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf27d15e45e4dd291a35143650bb0d0"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertScratch were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"metric = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:59:17.637649Z","iopub.execute_input":"2024-12-04T06:59:17.638082Z","iopub.status.idle":"2024-12-04T06:59:19.443949Z","shell.execute_reply.started":"2024-12-04T06:59:17.638047Z","shell.execute_reply":"2024-12-04T06:59:19.443304Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60711bf664384307b2a0bb6d5b20d7d5"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./bert_rdrop',  # output directory\n    num_train_epochs=3,  # total number of training epochs\n    per_device_train_batch_size=4,  # batch size per device during training\n    per_device_eval_batch_size=8,  # batch size for evaluation\n    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,  # strength of weight decay\n    logging_dir='./logs',  # directory for storing logs\n    logging_steps=100,\n    save_strategy=\"no\",\n    evaluation_strategy=\"epoch\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:59:38.996268Z","iopub.execute_input":"2024-12-04T06:59:38.997065Z","iopub.status.idle":"2024-12-04T06:59:39.099272Z","shell.execute_reply.started":"2024-12-04T06:59:38.997034Z","shell.execute_reply":"2024-12-04T06:59:39.098394Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,  # the instantiated 🤗 Transformers model to be trained\n    args=training_args,  # training arguments, defined above\n    train_dataset=tokenized_train,  # training dataset\n    eval_dataset=tokenized_val,  # evaluation dataset\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T06:59:54.415466Z","iopub.execute_input":"2024-12-04T06:59:54.416435Z","iopub.status.idle":"2024-12-04T06:59:56.134318Z","shell.execute_reply.started":"2024-12-04T06:59:54.416383Z","shell.execute_reply":"2024-12-04T06:59:56.133634Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2179846710.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction_outputs = trainer.predict(tokenized_test)\ntest_pred = np.argmax(prediction_outputs[0], axis=-1).flatten()\nprint(test_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": test_pred})\nresult_output.to_csv(\"../result/bert_rdrop.csv\", index=False, quoting=3)\nlogging.info('result saved!')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}