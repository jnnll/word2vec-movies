{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Word2vec是一种神经网络实现，用于学习词的分布式表达（distributed representations for words）。\n\nWord2vec即使不利用标签，也能产生有意义的表达。这是非常有用的，因为大部分真实世界里的数据是没有标签的。如果给的词足够多，词向量会展现很多有趣的特性。比如有相似意义的词会出现在一个类里，而不同的类是有间隔的，这种特性可以让词之间的关系，可以通过向量计算来表示。\n\n分布式词向量对于词预测和翻译很有用，这次我们用来做情感分析","metadata":{}},{"cell_type":"code","source":"! unzip /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n! unzip /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n! unzip /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:38:58.979457Z","iopub.execute_input":"2024-10-24T18:38:58.980024Z","iopub.status.idle":"2024-10-24T18:39:04.177835Z","shell.execute_reply.started":"2024-10-24T18:38:58.979980Z","shell.execute_reply":"2024-10-24T18:39:04.176484Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Archive:  /kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n  inflating: labeledTrainData.tsv    \nArchive:  /kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n  inflating: testData.tsv            \nArchive:  /kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n  inflating: unlabeledTrainData.tsv  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"首先，用pandas导入数据，不过这次我们用unlabeledTrain.tsv，其中包含了50000个没有标签的评论。在Part 1，训练词袋模型时，如果一个评论没有标签，那么这条数据就是没有用的。但word2vec能从没有标记的数据中学习。","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:05.744093Z","iopub.execute_input":"2024-10-24T18:39:05.744600Z","iopub.status.idle":"2024-10-24T18:39:06.238328Z","shell.execute_reply.started":"2024-10-24T18:39:05.744548Z","shell.execute_reply":"2024-10-24T18:39:06.237126Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/working/labeledTrainData.tsv\", header=0, \n                     delimiter=\"\\t\", quoting=3)\n\ntest = pd.read_csv( \"/kaggle/working/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv(\"/kaggle/working/unlabeledTrainData.tsv\", header=0, \n                              delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\\n\" % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size ))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:09.619907Z","iopub.execute_input":"2024-10-24T18:39:09.620545Z","iopub.status.idle":"2024-10-24T18:39:11.480867Z","shell.execute_reply.started":"2024-10-24T18:39:09.620497Z","shell.execute_reply":"2024-10-24T18:39:11.479713Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:16.313958Z","iopub.execute_input":"2024-10-24T18:39:16.314407Z","iopub.status.idle":"2024-10-24T18:39:16.331387Z","shell.execute_reply.started":"2024-10-24T18:39:16.314362Z","shell.execute_reply":"2024-10-24T18:39:16.330265Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         id  sentiment                                             review\n0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"5814_8\"</td>\n      <td>1</td>\n      <td>\"With all this stuff going down at the moment ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"2381_9\"</td>\n      <td>1</td>\n      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"7759_3\"</td>\n      <td>0</td>\n      <td>\"The film starts with a manager (Nicholas Bell...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"3630_4\"</td>\n      <td>0</td>\n      <td>\"It must be assumed that those who praised thi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\"9495_8\"</td>\n      <td>1</td>\n      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['review'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:18.904404Z","iopub.execute_input":"2024-10-24T18:39:18.905274Z","iopub.status.idle":"2024-10-24T18:39:18.912369Z","shell.execute_reply.started":"2024-10-24T18:39:18.905228Z","shell.execute_reply":"2024-10-24T18:39:18.911245Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"},"metadata":{}}]},{"cell_type":"markdown","source":"接下来对数据进行清洗，和Part1差不多，不过有些不一样的地方。首先，训练word2vec的时候，最好不要去除stop words，因为word2vec算法中，更多的词汇能产生更高质量的词向量，所以我们提供一个可选项。另外，最好不要去除数字：","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist(review, remove_stopwords=False):\n    # 定义一个函数，将评论转换为单词序列，可选择是否移除停用词。返回一个单词列表。\n    \n    # 1. 移除HTML标签\n    review_text = BeautifulSoup(review, 'lxml').get_text()\n      \n    # 2. 移除非字母字符\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n    # 3. 将所有单词转换为小写并分割它们\n    words = review_text.lower().split()\n\n    # 4. 可选择是否移除停用词（默认为False）\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    \n    # 5. 返回单词列表\n    return(words)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:22.284916Z","iopub.execute_input":"2024-10-24T18:39:22.286090Z","iopub.status.idle":"2024-10-24T18:39:22.512816Z","shell.execute_reply.started":"2024-10-24T18:39:22.286039Z","shell.execute_reply":"2024-10-24T18:39:22.511787Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"现在，我们想要规定好输入的格式。输入Word2vec的是单个句子，一个句子是一个list，由词组成。换句话说，输入格式是a list of lists一个列表的列表。\n\n想要把段落分割为句子并不是一件直观的事情。英语句子的结尾可以是\"?\", \"!\", \"\"\", or \".\", 而空格和大小写也靠不住。所以，我们将使用NLTK中的punkt标记生成器来进行句子分割。","metadata":{}},{"cell_type":"code","source":"import nltk.data\n# nltk.download()   \n\n# 加载punkt分词器\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# 定义一个函数，将评论分割成解析后的句子\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # 这个函数将评论分割成解析后的句子。返回一个句子列表，其中每个句子是一个单词列表\n    \n    # 1. 使用NLTK分词器将段落分割成句子\n    raw_sentences = tokenizer.tokenize(review.strip())\n    \n    # 2. 循环遍历每个句子\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # 如果一个句子为空，跳过它\n        if len(raw_sentence) > 0:\n            # 否则，调用review_to_wordlist函数来获取单词列表\n            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n\n    # 返回句子列表 每个句子是一个单词列表，因此，这返回一个列表的列表\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:28.319839Z","iopub.execute_input":"2024-10-24T18:39:28.321029Z","iopub.status.idle":"2024-10-24T18:39:28.328398Z","shell.execute_reply.started":"2024-10-24T18:39:28.320978Z","shell.execute_reply":"2024-10-24T18:39:28.327110Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# tokenizer只负责将paragraph分割为多个sentence\n# 对于每个sentence，review_to_wordlist负责对一个sentence进行清洗\ns1 = review_to_sentences(train['review'][0], tokenizer)\n# 输出第一个sentence的list\ns1[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:30.399995Z","iopub.execute_input":"2024-10-24T18:39:30.400497Z","iopub.status.idle":"2024-10-24T18:39:30.424625Z","shell.execute_reply.started":"2024-10-24T18:39:30.400449Z","shell.execute_reply":"2024-10-24T18:39:30.423347Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['with',\n 'all',\n 'this',\n 'stuff',\n 'going',\n 'down',\n 'at',\n 'the',\n 'moment',\n 'with',\n 'mj',\n 'i',\n 've',\n 'started',\n 'listening',\n 'to',\n 'his',\n 'music',\n 'watching',\n 'the',\n 'odd',\n 'documentary',\n 'here',\n 'and',\n 'there',\n 'watched',\n 'the',\n 'wiz',\n 'and',\n 'watched',\n 'moonwalker',\n 'again']"},"metadata":{}}]},{"cell_type":"markdown","source":"将训练集和未标记集（unlabeled set）中的评论文本解析成句子，并将这些句子存储在一个列表中","metadata":{}},{"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\nprint(\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint(\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:39:40.724764Z","iopub.execute_input":"2024-10-24T18:39:40.725363Z","iopub.status.idle":"2024-10-24T18:43:59.805347Z","shell.execute_reply.started":"2024-10-24T18:39:40.725305Z","shell.execute_reply":"2024-10-24T18:43:59.803929Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Parsing sentences from training set\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/630976817.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  review_text = BeautifulSoup(review, 'lxml').get_text()\n/tmp/ipykernel_30/630976817.py:9: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n  review_text = BeautifulSoup(review, 'lxml').get_text()\n","output_type":"stream"},{"name":"stdout","text":"Parsing sentences from unlabeled set\n","output_type":"stream"}]},{"cell_type":"markdown","source":"检查 sentences 列表中句子的总数：","metadata":{}},{"cell_type":"code","source":"# Check how many sentences we have in total - should be around 850,000+\nlen(sentences)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:44:14.640324Z","iopub.execute_input":"2024-10-24T18:44:14.641384Z","iopub.status.idle":"2024-10-24T18:44:14.648508Z","shell.execute_reply.started":"2024-10-24T18:44:14.641333Z","shell.execute_reply":"2024-10-24T18:44:14.647152Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"795538"},"metadata":{}}]},{"cell_type":"code","source":"print(sentences[0])\n\nprint(sentences[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:44:16.649280Z","iopub.execute_input":"2024-10-24T18:44:16.650389Z","iopub.status.idle":"2024-10-24T18:44:16.656087Z","shell.execute_reply.started":"2024-10-24T18:44:16.650334Z","shell.execute_reply":"2024-10-24T18:44:16.654913Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**训练和保存模型**","metadata":{}},{"cell_type":"code","source":"# 导入内置的logging模块并配置它 以便Word2Vec创建友好的输出消息\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# 设置各种参数的值\nnum_features = 300    # 词向量的维度                     \nmin_word_count = 40   # 最低词频                        \nnum_workers = 4       # 并行运行的线程数量\ncontext = 10          # 上下文窗口大小                                                                                  \ndownsampling = 1e-3   # 频繁词的降采样设置\n\n# 初始化并训练模型\nfrom gensim.models import word2vec\nprint(\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \n            vector_size=num_features, min_count = min_word_count, \n            window = context, sample = downsampling)\n\n# 如果你不打算进一步训练模型，调用init_sims会使模型更加内存高效\nmodel.init_sims(replace=True)\n\n# 创建一个有意义的模型名称并保存模型以供以后使用。你可以以后使用Word2Vec.load()来加载它\nmodel_name = \"/kaggle/working/300features_40minwords_10context\"\nmodel.save(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:44:34.199590Z","iopub.execute_input":"2024-10-24T18:44:34.200037Z","iopub.status.idle":"2024-10-24T18:46:52.246807Z","shell.execute_reply.started":"2024-10-24T18:44:34.199997Z","shell.execute_reply":"2024-10-24T18:46:52.245672Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/1333739766.py:20: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n  model.init_sims(replace=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**探索模型结果**\n\n训练结束后，查看75000个评论的训练结果。doesnt_match函数会推断在一个集合里，哪一个单词与其他单词最不相似：","metadata":{}},{"cell_type":"code","source":"model.wv.doesnt_match('man woman child kitchen'.split())","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:47:34.699567Z","iopub.execute_input":"2024-10-24T18:47:34.700031Z","iopub.status.idle":"2024-10-24T18:47:34.709497Z","shell.execute_reply.started":"2024-10-24T18:47:34.699988Z","shell.execute_reply":"2024-10-24T18:47:34.708215Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'kitchen'"},"metadata":{}}]},{"cell_type":"code","source":"model.wv.doesnt_match('france england germany berlin'.split())","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:47:36.599068Z","iopub.execute_input":"2024-10-24T18:47:36.600451Z","iopub.status.idle":"2024-10-24T18:47:36.608712Z","shell.execute_reply.started":"2024-10-24T18:47:36.600381Z","shell.execute_reply":"2024-10-24T18:47:36.607464Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'berlin'"},"metadata":{}}]},{"cell_type":"code","source":"model.wv.doesnt_match('paris berlin london austria'.split())","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:47:38.404542Z","iopub.execute_input":"2024-10-24T18:47:38.405121Z","iopub.status.idle":"2024-10-24T18:47:38.414782Z","shell.execute_reply.started":"2024-10-24T18:47:38.405061Z","shell.execute_reply":"2024-10-24T18:47:38.413449Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'paris'"},"metadata":{}}]},{"cell_type":"markdown","source":"用most_similar函数来查看词汇集群：","metadata":{}},{"cell_type":"code","source":"model.wv.most_similar('man')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:47:40.534407Z","iopub.execute_input":"2024-10-24T18:47:40.535424Z","iopub.status.idle":"2024-10-24T18:47:40.557530Z","shell.execute_reply.started":"2024-10-24T18:47:40.535372Z","shell.execute_reply":"2024-10-24T18:47:40.555935Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[('woman', 0.6229605674743652),\n ('lady', 0.601617693901062),\n ('lad', 0.5919666290283203),\n ('monk', 0.5295251607894897),\n ('millionaire', 0.5230624079704285),\n ('men', 0.514593780040741),\n ('soldier', 0.5063545107841492),\n ('guy', 0.49768635630607605),\n ('person', 0.4933617115020752),\n ('sailor', 0.4897652864456177)]"},"metadata":{}}]},{"cell_type":"markdown","source":"对情感分析做测试：","metadata":{}},{"cell_type":"code","source":"model.wv.most_similar('awful')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:47:43.174887Z","iopub.execute_input":"2024-10-24T18:47:43.175359Z","iopub.status.idle":"2024-10-24T18:47:43.194802Z","shell.execute_reply.started":"2024-10-24T18:47:43.175315Z","shell.execute_reply":"2024-10-24T18:47:43.193238Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[('terrible', 0.7640948295593262),\n ('horrible', 0.7426056265830994),\n ('atrocious', 0.7376793622970581),\n ('dreadful', 0.7110174298286438),\n ('abysmal', 0.6884176731109619),\n ('appalling', 0.6729736924171448),\n ('horrid', 0.6491310000419617),\n ('horrendous', 0.6484755873680115),\n ('lousy', 0.6404727101325989),\n ('amateurish', 0.6080129742622375)]"},"metadata":{}}]}]}